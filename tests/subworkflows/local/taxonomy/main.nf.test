nextflow_workflow {

    name "Test subworkflow TAXONOMY"
    script "subworkflows/local/taxonomy/main.nf"
    workflow "TAXONOMY"
    tag "subworkflow"
    tag "taxonomy"

    test("Should run without failures on paired (interleaved) input") {
        tag "expect_success"
        tag "interleaved"
        config "tests/configs/run.config"
        setup {
            run("LOAD_SAMPLESHEET") {
                script "subworkflows/local/loadSampleSheet/main.nf"
                process {
                    """
                    input[0] = "${projectDir}/test-data/samplesheet.csv"
                    input[1] = "illumina"
                    input[2] = false
                    """
                }
            }
            run("INTERLEAVE_FASTQ") {
                script "modules/local/interleaveFastq/main.nf"
                process {
                    """
                    input[0] = LOAD_SAMPLESHEET.out.samplesheet
                    """
                }
            }
        }
        when {
            params {
            }
            workflow {
                '''
                def taxonomy_params = [
                    classification_level: "D",
                    bracken_threshold: "5"
                ]
                input[0] = INTERLEAVE_FASTQ.out.output
                input[1] = "${params.ref_dir}/results/kraken_db"
                input[2] = taxonomy_params
                input[3] = LOAD_SAMPLESHEET.out.single_end
                '''
            }
        }
        then {
            // Should run without failures
            assert workflow.success
            // Read output should have a nonzero number of lines divisible by four
            def countGzipLines = { file -> path(file).linesGzip.size() }
            def single_read_lines = countGzipLines(workflow.out.single_reads[0][1])
            assert single_read_lines > 0
            assert single_read_lines % 4 == 0
            // Merged read count should equal input read count
            def input_read_lines = countGzipLines(workflow.out.input_reads[0][1])
            assert single_read_lines == input_read_lines / 2
            // Kraken output lines should match input reads
            def kraken_output_lines = countGzipLines(workflow.out.kraken_output[0][1])
            assert kraken_output_lines == single_read_lines / 4
            // Kraken reports should have expected fields
            def kraken_report_tab = path(workflow.out.kraken_reports[0]).csv(sep: "\t", decompress: true)
            def kraken_headers_exp = ["pc_reads_total", "n_reads_clade", "n_reads_direct", "n_minimizers_total", "n_minimizers_distinct", "rank","taxid", "name", "sample"]
            assert kraken_report_tab.columnCount == 9
            assert kraken_report_tab.columnNames == kraken_headers_exp
            // Bracken output should have expected fields
            def bracken_tab = path(workflow.out.bracken[0]).csv(sep: "\t", decompress: true)
            def bracken_headers_exp = ["name", "taxid", "rank", "kraken2_assigned_reads", "added_reads", "new_est_reads", "fraction_total_reads", "sample"]
            assert bracken_tab.columnCount == 8
            assert bracken_tab.columnNames == bracken_headers_exp
            // BBMerge summary output should have expected fields
            def bbmerge_tab = path(workflow.out.bbmerge_summary[0][1]).csv(sep: "\t", decompress: true)
            def bbmerge_headers_exp = ["seq_id", "bbmerge_frag_length"]
            assert bbmerge_tab
            assert bbmerge_tab.columnNames == bbmerge_headers_exp
        }
    }

    test("Should run without failures on unpaired input") {
        tag "expect_success"
        tag "single_end"
        config "tests/configs/run.config"
        setup {
            run("LOAD_SAMPLESHEET") {
                script "subworkflows/local/loadSampleSheet/main.nf"
                process {
                    """
                    input[0] = "${projectDir}/test-data/single-end-samplesheet.csv"
                    input[1] = "illumina"
                    input[2] = true
                    """
                }
            }
            run("COPY_FILE") {
                script "modules/local/copyFile/main.nf"
                process {
                    """
                    input[0] = LOAD_SAMPLESHEET.out.samplesheet
                    input[1] = "reads.fastq.gz"
                    """
                }
            }
        }
        when {
            params {}
            workflow {
                '''
                def taxonomy_params = [
                    classification_level: "D",
                    bracken_threshold: "5",
                    single_end: true
=
                ]
                input[0] = COPY_FILE.out
                input[1] = "${params.ref_dir}/results/kraken_db"
                input[2] = taxonomy_params
                // input[3] = LOAD_SAMPLESHEET.out.single_end
                '''
            }
        }
        then {
            // Should run without failures
            assert workflow.success
            // Read output should have a nonzero number of lines divisible by four
            def countGzipLines = { file -> path(file).linesGzip.size() }
            def single_read_lines = countGzipLines(workflow.out.single_reads[0][1])
            assert single_read_lines > 0
            assert single_read_lines % 4 == 0
            // "Merged" reads should exactly equal input reads
            def input_read_md5 = path(workflow.out.input_reads[0][1]).md5
            def single_read_md5 = path(workflow.out.single_reads[0][1]).md5
            assert single_read_md5 == input_read_md5
            def input_read_lines = countGzipLines(workflow.out.input_reads[0][1])
            assert single_read_lines == input_read_lines
            // Kraken output lines should match input reads
            def kraken_output_lines = countGzipLines(workflow.out.kraken_output[0][1])
            assert kraken_output_lines == single_read_lines / 4
            // Kraken reports should have expected fields
            def kraken_report_tab = path(workflow.out.kraken_reports[0]).csv(sep: "\t", decompress: true)
            def kraken_headers_exp = ["pc_reads_total", "n_reads_clade", "n_reads_direct", "n_minimizers_total", "n_minimizers_distinct", "rank","taxid", "name", "sample"]
            assert kraken_report_tab.columnCount == 9
            assert kraken_report_tab.columnNames == kraken_headers_exp
            // Bracken output should have expected fields
            def bracken_tab = path(workflow.out.bracken[0]).csv(sep: "\t", decompress: true)
            def bracken_headers_exp = ["name", "taxid", "rank", "kraken2_assigned_reads", "added_reads", "new_est_reads", "fraction_total_reads", "sample"]
            assert bracken_tab.columnCount == 8
            assert bracken_tab.columnNames == bracken_headers_exp
            // BBMerge summary output should be empty
            assert workflow.out.bbmerge_summary == []
        }
    }

    test("Should handle empty input file") {
        tag "empty_file"
        tag "expect_success"
        config "tests/configs/run.config"
        setup {
            run("GZIP_FILE", alias: "GZIP_EMPTY") {
                script "modules/local/gzipFile/main.nf"
                process {
                    '''
                    input[0] = Channel.of("test_empty")
                        | combine(Channel.of("${projectDir}/test-data/toy-data/empty_file.txt"))
                    '''
                }
            }
        }
        when {
            params {}
            workflow {
                '''
                def taxonomy_params = [
                    classification_level: "D",
                    bracken_threshold: "5"
                ]
                input[0] = GZIP_EMPTY.out
                input[1] = "${params.ref_dir}/results/kraken_db"
                input[2] = taxonomy_params
                input[3] = Channel.of(true)
                '''
            }
        }
        then {
            // Should run without failures
            assert workflow.success
            
            // Single reads should be empty
            def countGzipLines = { file -> path(file).linesGzip.size() }
            def single_read_lines = countGzipLines(workflow.out.single_reads[0][1])
            assert single_read_lines == 0
            
            // Kraken output should have only header row
            def kraken_output_lines = countGzipLines(workflow.out.kraken_output[0][1])
            assert kraken_output_lines == 0
            
            // Kraken reports should have header row only
            def kraken_report_file = path(workflow.out.kraken_reports[0])
            def kraken_lines = kraken_report_file.linesGzip.toList()
            assert kraken_lines.size() == 1  // Just header
            
            // Check header columns
            def kraken_headers = kraken_lines[0].split("\t")
            def kraken_headers_exp = ["pc_reads_total", "n_reads_clade", "n_reads_direct", "n_minimizers_total", "n_minimizers_distinct", "rank","taxid", "name", "sample"]
            assert kraken_headers.size() == kraken_headers_exp.size()
            
            // Bracken output should be completely empty
            def bracken_file = path(workflow.out.bracken[0])
            assert bracken_file.linesGzip.size() == 0
        }
    }
}
