nextflow_workflow {

    name "Test workflow CONCATENATE_TSVS_ACROSS_SELECTED_TAXID"
    script "subworkflows/local/concatenateTsvsAcrossSelectedTaxid/main.nf"
    workflow "CONCATENATE_TSVS_ACROSS_SELECTED_TAXID"
    config "tests/configs/downstream.config"
    tag "subworkflow"
    tag "downstream"
    tag "concatenate_tsvs_across_species"

    test("Should correctly handle valid input data") {
        tag "expect_success"
        setup {
            run("LOAD_DOWNSTREAM_DATA") {
                script "subworkflows/local/loadDownstreamData/main.nf"
                process {
                    """
                    input[0] = "${projectDir}/test-data/downstream/input_file_single.csv"
                    """
                }
            }
            run("PREPARE_GROUP_TSVS") {
                script "subworkflows/local/prepareGroupTsvs/main.nf"
                process {
                    '''
                    input[0] = LOAD_DOWNSTREAM_DATA.out.input
                    '''
                }
            }
            run("COPY_FILE_BARE") {
                script "modules/local/copyFile/main.nf"
                process {
                    '''
                    input[0] = "${params.ref_dir}/results/total-virus-db-annotated.tsv.gz"
                    input[1] = "virus-db.tsv.gz"
                    '''
                }
            }
            run("SPLIT_VIRAL_TSV_BY_SELECTED_TAXID") {
                script "subworkflows/local/splitViralTsvBySelectedTaxid/main.nf"
                process {
                    '''
                    input[0] = PREPARE_GROUP_TSVS.out.groups // groups
                    input[1] = COPY_FILE_BARE.out // db
                    '''
                }
            }
        }
        when {
            params {
            }
            workflow {
                '''
                input[0] = SPLIT_VIRAL_TSV_BY_SELECTED_TAXID.out.tsv // tsvs
                input[1] = "test" // filename_suffix
                '''
            }
        }
        then {
            // Should run without failures
            assert workflow.success
            // Should correctly group and concatenate TSVs
            def labels_in = workflow.out.test_input.collect{it[0]}
            def groups_in_raw = labels_in.collect{it.tokenize("_")[0..-2].join("_")}
            def groups_in = groups_in_raw.toSorted().unique()
            def labels_out = workflow.out.output.collect{it[0]}
            assert groups_in.size() == labels_out.size()
            assert groups_in == labels_out
            // Concatenated TSVs should have the correct number of columns
            def tabs_in = workflow.out.test_input.collect{path(it[1]).csv(sep: "\t", decompress: true)}
            def tabs_out = workflow.out.output.collect{path(it[1]).csv(sep: "\t", decompress: true)}
            def n_cols = tabs_in[0].columnCount
            for (int i=0; i < tabs_in.size(); i++) {
                assert tabs_in[i].columnCount == n_cols
            }
            // Concatenated TSVs should have the correct number of rows
            def n_rows = [:]
            for (int i=0; i < tabs_in.size(); i++) {
                def group = groups_in_raw[i]
                if (n_rows[group] == null) {
                    n_rows[group] = tabs_in[i].rowCount
                } else {
                    n_rows[group] += tabs_in[i].rowCount
                }
            }
            for (int i=0; i < tabs_out.size(); i++) {
                assert tabs_out[i].rowCount == n_rows[groups_in_raw[i]]
            }
        }
    }

}
