nextflow_workflow {

    name "Test subworkflow EXTRACT_VIRAL_READS_ONT_LCA"
    script "subworkflows/local/extractViralReadsONTLca/main.nf"
    workflow "EXTRACT_VIRAL_READS_ONT_LCA"
    config "tests/configs/run_ont.config"
    tag "subworkflow"
    tag "extract_viral_reads_ont_lca"

    setup {
        run("LOAD_SAMPLESHEET") {
            script "subworkflows/local/loadSampleSheet/main.nf"
            process {
                """
                input[0] = "${projectDir}/test-data/ont-samplesheet.csv"
                input[1] = "ont"
                input[2] = false
                """
            }
        }
    }

    test("Should run without failures") {
        tag "expect_success"
        tag "ont_lca"
        tag "harmon-test"
        when {
            workflow {
                '''
                input[0] = LOAD_SAMPLESHEET.out.samplesheet
                input[1] = params.ref_dir
                input[2] = "81077"
                '''
            }
        }
        then {
            // Should run without failures
            assert workflow.success

            // Viral minimap2 output should have the same read IDS as the final vertebrate viral TSV
            def samlines = sam(workflow.out.test_minimap2_virus[0][1]).getSamLines()
            def sam_ids = samlines
                .collect { line -> line.split('\t')[0] }  // Get read IDs
                .toSet()
            def hits_final = path(workflow.out.hits_final[0]).csv(sep: "\t", decompress: true)
            def hits_final_read_ids_list = hits_final.columns["seq_id"]  // Keep as list to check for duplicates
            def hits_final_read_ids_set = hits_final_read_ids_list.toSet()
            
            // Verify there's only one entry for each read ID in the final output
            assert hits_final_read_ids_list.size() == hits_final_read_ids_set.size(), 
                "Found duplicate read IDs in final output: expected ${hits_final_read_ids_set.size()} unique IDs but got ${hits_final_read_ids_list.size()} total entries"
            
            // Verify all final read IDs exist in the SAM file
            assert sam_ids.size() > 0
            assert hits_final_read_ids_set.every { it in sam_ids }, 
                "All read IDs in final output should exist in SAM file"

            // Check the the human and contam filtering work and produce FASTQ files that are subsets of the prior FASTQ file.
            def fastq_filtered_human = path(workflow.out.test_fastq_filtered_human[0][1]).fastq
            def fastq_filtered_contam = path(workflow.out.test_fastq_filtered_contam[0][1]).fastq
            def fastq_viral_hits = path(workflow.out.hits_fastq[0]).fastq
            def human_filter_num = fastq_filtered_human.readNames.size()
            def contam_filter_num = fastq_filtered_contam.readNames.size()
            def virus_filter_num = fastq_viral_hits.readNames.size()

            assert human_filter_num > contam_filter_num
            assert contam_filter_num > virus_filter_num
            assert virus_filter_num > 0
        }
    }

    test("Should handle empty input file") {
        tag "empty_file"
        tag "expect_success"
        tag "ont_lca"
        setup {
            run("GZIP_FILE", alias: "GZIP_EMPTY_FORWARD") {
                script "modules/local/gzipFile/main.nf"
                process {
                    '''
                    input[0] = Channel.of("empty")
                        | combine(Channel.of("${projectDir}/test-data/toy-data/empty_file.txt"))
                    '''
                }
            }
        }
        when {
            
            workflow {
                '''
                input[0] = GZIP_EMPTY_FORWARD.out
                input[1] = params.ref_dir
                input[2] = "81077"
                '''
            }
        }
        then {
            // Should run without failures
            assert workflow.success

            // FASTQ and SAM output files should be empty
            def fastq_files = [
                path(workflow.out.hits_fastq[0]),
                path(workflow.out.test_fastq_filtered_human[0][1]),
                path(workflow.out.test_fastq_filtered_contam[0][1])
            ]
            def fastqs = fastq_files.collect{ it.fastq }
            def read_counts = fastqs.collect{ it.getNumberOfRecords() }
            assert read_counts.every{ it == 0 }
            def samlines = sam(workflow.out.test_minimap2_virus[0][1]).getSamLines()
	    assert samlines.size() == 0
            // Tabular output files should have only header lines
            def tabular_files = [
                path(workflow.out.hits_final[0]),
            ]
            def tabular_lines = tabular_files.collect{ it.linesGzip.toList() }
            assert tabular_lines.every{ it.size() == 1 }
        }
    }
}
