nextflow_workflow {

    name "Test workflow VALIDATE_VIRAL_ASSIGNMENTS"
    script "subworkflows/local/validateViralAssignments/main.nf"
    workflow "VALIDATE_VIRAL_ASSIGNMENTS"
    config "tests/configs/downstream.config"
    tag "subworkflow"
    tag "downstream"
    tag "validate_viral_assignments"

    test("Should fail when input TSV contains duplicate read IDs") {
        tag "expect_failed"
        tag "duplicate_input"
        setup {
            run("LOAD_DOWNSTREAM_DATA") {
                script "subworkflows/local/loadDownstreamData/main.nf"
                process {
                    """
                    input[0] = "${projectDir}/test-data/downstream/input_file.csv"
                    """
                }
            }
            run("PREPARE_GROUP_TSVS") {
                script "subworkflows/local/prepareGroupTsvs/main.nf"
                process {
                    '''
                    input[0] = LOAD_DOWNSTREAM_DATA.out.input
                    '''
                }
            }
            run("COPY_FILE_BARE") {
                script "modules/local/copyFile/main.nf"
                process {
                    '''
                    input[0] = "${params.ref_dir}/results/total-virus-db-annotated.tsv.gz"
                    input[1] = "virus-db.tsv.gz"
                    '''
                }
            }
        }
        when {
            params {
                n_clusters = 3
                taxid_artificial = 81077
            }
            workflow {
                '''
                def validation_params = [
                    validation_cluster_identity: params.validation_cluster_identity,
                    cluster_min_len: 15,
                    validation_n_clusters: params.n_clusters,
                    blast_db_prefix: params.blast_db_prefix,
                    blast_perc_id: params.blast_perc_id,
                    blast_qcov_hsp_perc: params.blast_qcov_hsp_perc,
                    blast_max_rank: params.blast_max_rank,
                    blast_min_frac: params.blast_min_frac,
                    taxid_artificial: params.taxid_artificial,
                    db_download_timeout: params.db_download_timeout
                ]
                input[0] = PREPARE_GROUP_TSVS.out.groups // groups
                input[1] = COPY_FILE_BARE.out // db
                input[2] = params.ref_dir
                input[3] = validation_params
                '''
            }
        }
        then {
            // Should run without failures
            assert workflow.failed
            assert workflow.exitStatus == 1
            assert workflow.errorReport.contains("ValueError: Duplicate value found in field seq_id")
        }
    }

    test("Should run without failures on valid input data") {
        tag "expect_success"
        setup {
            run("LOAD_DOWNSTREAM_DATA") {
                script "subworkflows/local/loadDownstreamData/main.nf"
                process {
                    """
                    input[0] = "${projectDir}/test-data/downstream/input_file_single.csv"
                    """
                }
            }
            run("PREPARE_GROUP_TSVS") {
                script "subworkflows/local/prepareGroupTsvs/main.nf"
                process {
                    '''
                    input[0] = LOAD_DOWNSTREAM_DATA.out.input
                    '''
                }
            }
            run("COPY_FILE_BARE") {
                script "modules/local/copyFile/main.nf"
                process {
                    '''
                    input[0] = "${params.ref_dir}/results/total-virus-db-annotated.tsv.gz"
                    input[1] = "virus-db.tsv.gz"
                    '''
                }
            }
        }
        when {
            params {
                n_clusters = 3
                taxid_artificial = 81077
            }
            workflow {
                '''
                def validation_params = [
                    validation_cluster_identity: params.validation_cluster_identity,
                    cluster_min_len: 15,
                    validation_n_clusters: params.n_clusters,
                    blast_db_prefix: params.blast_db_prefix,
                    blast_perc_id: params.blast_perc_id,
                    blast_qcov_hsp_perc: params.blast_qcov_hsp_perc,
                    blast_max_rank: params.blast_max_rank,
                    blast_min_frac: params.blast_min_frac,
                    taxid_artificial: params.taxid_artificial,
                    db_download_timeout: params.db_download_timeout
                ]
                input[0] = PREPARE_GROUP_TSVS.out.groups // groups
                input[1] = COPY_FILE_BARE.out // db
                input[2] = params.ref_dir
                input[3] = validation_params
                '''
            }
        }
        then {
            // Should run without failures
            assert workflow.success
            // Clustering should preserve reads
            def tabs_split = workflow.out.test_split_tsv.collect{path(it[1]).csv(sep: "\t",decompress: true)}
            def tabs_clust = workflow.out.test_cluster_tab.collect{path(it[1]).csv(sep: "\t",decompress: true)}
            assert tabs_split.size() == tabs_clust.size()
            for (int i = 0; i < tabs_split.size(); i++) {
                assert tabs_split[i].rowCount == tabs_clust[i].rowCount
            }
            // Concatenation should work correctly
            def cluster_fasta = workflow.out.test_reps_fasta.collect{path(it[1]).fasta}
            def concat_fasta = workflow.out.test_concat_fasta.collect{path(it[1]).fasta}
            def cluster_labels = workflow.out.test_reps_fasta.collect{it[0]}
            def groups_from_species = cluster_labels.collect { label ->
                def pattern = /^(.*?)_(\d+)$/
                def matcher = (label =~ pattern)
                return matcher[0][1]  // Extract group part
            }.unique()
            // Concatenated data should have exactly one file per unique group
            assert concat_fasta.size() == groups_from_species.size()
            def concat_labels = workflow.out.test_concat_fasta.collect{it[0]}
            assert concat_labels.toSet() == groups_from_species.toSet()
            // For each group, verify all species sequences are concatenated
            groups_from_species.each { group ->
                // Find all species files for this group
                def species_files = cluster_fasta.withIndex().findAll { fasta, idx ->
                    cluster_labels[idx].startsWith("${group}_")
                }.collect { it[0] }
                
                // Find concatenated file for this group
                def concat_file = concat_fasta[concat_labels.indexOf(group)]
                
                // Total sequences should match
                def species_seq_count = species_files.sum { it.keySet().size() }
                def concat_seq_count = concat_file.keySet().size()
                assert species_seq_count == concat_seq_count
            }
            // Basic BLAST invocation should work as expected
            def blast_db = workflow.out.test_blast_db.collect{path(it[1]).csv(sep: "\t",decompress: true)}
            def blast_query = workflow.out.test_blast_query.collect{path(it[1]).fasta}
            def blast_lca = workflow.out.test_blast_lca.collect{path(it[1]).csv(sep: "\t",decompress: true)}
            assert blast_query == concat_fasta
            assert blast_db.size() == blast_query.size()
            // BLAST now runs on concatenated data, so we have 1 job per group instead of 1 per group/species
            for (int i=0; i < blast_db.size(); i++) {
                if (blast_db[i].rowCount == 0) {
                    continue
                }
                // qseqids should all be in query input
                def ids_in = blast_query[i].keySet().collect{ it.tokenize(" ")[0] }.toSet()
                def ids_out = blast_db[i].columns["qseqid"].toSet()
                assert ids_in.containsAll(ids_out)
                // LCA output should have one row per sequence in BLAST output
                assert blast_lca[i].rowCount == blast_db[i].columns["qseqid"].toSet().size()
            }
            // Taxid validation should work as expected
            def tabs_validate = workflow.out.test_validate.collect{path(it[1]).csv(sep: "\t", decompress: true)}
            assert tabs_validate.size() == blast_lca.size()
            for (int i=0; i < tabs_validate.size(); i++) {
                if (tabs_validate[i].rowCount > 0) {
                    assert tabs_validate[i].columns["vsearch_cluster_rep_id"].unique().toSorted() == blast_lca[i].columns["qseqid"].unique().toSorted()
                }
            }
            // Propagation should work as expected (now working on group-level data)
            def tabs_propagate = workflow.out.test_propagate.collect{path(it[1]).csv(sep: "\t", decompress: true)}
            def tabs_in = workflow.out.test_in.collect{path(it[1]).csv(sep: "\t",decompress: true)}
            def concat_cluster = workflow.out.test_concat_cluster.collect{path(it[1]).csv(sep: "\t", decompress: true)}
            assert tabs_propagate.size() == tabs_validate.size()
            for (int i=0; i < tabs_propagate.size(); i++) {
                assert tabs_propagate[i].rowCount == tabs_in[i].rowCount
                assert tabs_propagate[i].columns["seq_id"].toSorted() == tabs_in[i].columns["seq_id"].toSorted()
                if (tabs_validate[i].rowCount > 0) {
                    def col_exp = tabs_in[i].columnNames.toSet() + tabs_validate[i].columnNames.toSet() + concat_cluster[i].columnNames.toSet()
                    def ncol_exp = col_exp.size()
                    assert tabs_propagate[i].columnCount == ncol_exp
                }
            }
            // Regrouping should work as expected
            def tabs_regrouped = workflow.out.annotated_hits.collect{path(it[1]).csv(sep: "\t",decompress: true)}
            assert tabs_regrouped.size() == tabs_in.size()
            for (int i=0; i < tabs_regrouped.size(); i++) {
                assert tabs_regrouped[i].rowCount == tabs_in[i].rowCount
                assert tabs_regrouped[i].columns["seq_id"].unique().toSorted() == tabs_in[i].columns["seq_id"].unique().toSorted()
            }
        }
    }

}
