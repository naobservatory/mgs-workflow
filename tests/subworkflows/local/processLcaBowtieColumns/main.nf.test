nextflow_workflow {

    name "Test Workflow PROCESS_LCA_BOWTIE_COLUMNS"
    script "subworkflows/local/processLcaBowtieColumns/main.nf"
    workflow "PROCESS_LCA_BOWTIE_COLUMNS"
    config "tests/configs/run.config"
    tag "subworkflow"
    tag "process_lca_bowtie_columns"

    test("Should run without failures") {
        tag "expect_success"
        setup {
            run("LOAD_SAMPLESHEET") {
                script "subworkflows/local/loadSampleSheet/main.nf"
                process {
                    """
                    input[0] = "${projectDir}/test-data/samplesheet.csv"
                    input[1] = "illumina"
                    input[2] = false
                    """
                }
            }
            run("INTERLEAVE_FASTQ") {
                script "modules/local/interleaveFastq/main.nf"
                process {
                    '''
                    input[0] = LOAD_SAMPLESHEET.out.samplesheet
                    '''
                }
            }
            run("BOWTIE2") {
                    script "modules/local/bowtie2/main.nf"
                    process {
                        '''
                        input[0] = INTERLEAVE_FASTQ.out.output 
                        input[1] = "${params.ref_dir}/results/bt2-virus-index"
                        input[2] = "--local --very-sensitive-local --score-min G,0.1,19"
                        input[3] = "virus"
                        input[4] = true
                        input[5] = false
                        input[6] = true
                        '''
                    }
                }
            run("SORT_FILE") {
                script "modules/local/sortFile/main.nf"
                process {
                    '''
                    input[0] = BOWTIE2.out.sam
                    input[1] = "-t\\$\'\\t\' -k1,1 -k2,2n"
                    input[2] = "sam"
                    '''
                }
            }
            run("PROCESS_VIRAL_BOWTIE2_SAM_LCA") {
                script "modules/local/processViralBowtie2SamLca/main.nf"
                process {
                    '''
                    input[0] = SORT_FILE.out.output
                    input[1] = "${params.ref_dir}/results/virus-genome-metadata-gid.tsv.gz"
                    input[2] = "${params.ref_dir}/results/total-virus-db-annotated.tsv.gz"
                    input[3] = true
                    '''
                }
            }
            run("LCA_TSV") {
                script "modules/local/lcaTsv/main.nf"
                process {
                    '''
                    input[0] = PROCESS_VIRAL_BOWTIE2_SAM_LCA.out.output
                    input[1] = "${params.ref_dir}/results/taxonomy-nodes.dmp"
                    input[2] = "${params.ref_dir}/results/taxonomy-names.dmp"
                    input[3] = "seq_id"
                    input[4] = "taxid"
                    input[5] = "length_normalized_score"
                    input[6] = "12908"
                    input[7] = "aligner"
                    '''
                }
            }

        }
        when {
            params {
                col_keep_no_prefix = ["seq_id", "aligner_taxid_lca"]
                col_keep_add_prefix = ["genome_id_all"]
                column_prefix = "prim_align_"
            }
            workflow {
                """
                input[0] = LCA_TSV.out.output
                input[1] = PROCESS_VIRAL_BOWTIE2_SAM_LCA.out.output
                input[2] = params.col_keep_no_prefix
                input[3] = params.col_keep_add_prefix
                input[4] = params.column_prefix
                """
            }
        }

        then {
          assert workflow.success

          // Parse input files using nf-test CSV helpers
          def lcaData = path(workflow.out.test_lca[0][1]).csv(sep: "\t", decompress: true)
          def bowtieData = path(workflow.out.test_bowtie[0][1]).csv(sep: "\t", decompress: true)
          def outputData = path(workflow.out.output[0][1]).csv(sep: "\t", decompress: true)

          // Basic structure validation
          assert outputData.columnCount > 0
          assert outputData.rowCount > 0

          // Validate expected columns exist
          assert "seq_id" in outputData.columns.keySet()
          assert "aligner_taxid_lca" in outputData.columns.keySet()
          assert "prim_align_genome_id_all" in outputData.columns.keySet()
          assert outputData.columnCount == 3

          // Validate join integrity - all output seq_ids should exist in both inputs
          def outputSeqIds = outputData.columns["seq_id"].toSet()
          def lcaSeqIds = lcaData.columns["seq_id"].toSet()
          def bowtieSeqIds = bowtieData.columns["seq_id"].toSet()

          for (seqId in outputSeqIds) {
              assert seqId in lcaSeqIds
              assert seqId in bowtieSeqIds
          }

          // Validate filtering - output should only contain primary alignments
          // Need to iterate through rows by index to filter
          def primaryBowtieSeqIds = []
          for (int i = 0; i < bowtieData.rowCount; i++) {
              if (bowtieData.columns["classification"][i] == "primary") {
                  primaryBowtieSeqIds.add(bowtieData.columns["seq_id"][i])
              }
          }
          def primaryBowtieSeqIdsSet = primaryBowtieSeqIds.toSet()

          // Output seq_ids should be intersection of primary alignments and LCA data
          def expectedSeqIds = primaryBowtieSeqIdsSet.intersect(lcaSeqIds)
          assert outputSeqIds == expectedSeqIds

          // Validate data integrity - values should match between input and output
          for (int i = 0; i < outputData.rowCount; i++) {
              def seqId = outputData.columns["seq_id"][i]

              // Find corresponding rows in input files by index
              def lcaIndex = lcaData.columns["seq_id"].findIndexOf { it == seqId }
              def bowtieIndex = bowtieData.columns["seq_id"].findIndexOf { it == seqId }

              // Check LCA data preserved
              assert outputData.columns["aligner_taxid_lca"][i] == lcaData.columns["aligner_taxid_lca"][lcaIndex]

              // Check Bowtie data preserved with renamed column
              assert outputData.columns["prim_align_genome_id_all"][i] == bowtieData.columns["genome_id_all"][bowtieIndex]
          }
        }
    }

    test("Should handle empty input files") {
        tag "empty_input"
        tag "expect_success"
        
        setup {
            run("COPY_FILE_BARE", alias: "COPY_LCA_EMPTY") {
                script "modules/local/copyFile/main.nf"
                process {
                    '''
                    input[0] = "${projectDir}/test-data/toy-data/empty_file.txt"
                    input[1] = "empty_lca.txt"
                    '''
                }
            }
            run("GZIP_FILE", alias: "GZIP_LCA_EMPTY") {
                script "modules/local/gzipFile/main.nf"
                process {
                    '''
                    input[0] = Channel.of("test")
                        | combine(COPY_LCA_EMPTY.out)
                    '''
                }
            }
            run("COPY_FILE_BARE", alias: "COPY_BOWTIE_EMPTY") {
                script "modules/local/copyFile/main.nf"
                process {
                    '''
                    input[0] = "${projectDir}/test-data/toy-data/empty_file.txt"
                    input[1] = "empty_bowtie.txt"
                    '''
                }
            }
            run("GZIP_FILE", alias: "GZIP_BOWTIE_EMPTY") {
                script "modules/local/gzipFile/main.nf"
                process {
                    '''
                    input[0] = Channel.of("test")
                        | combine(COPY_BOWTIE_EMPTY.out)
                    '''
                }
            }
        }

        when {
            params {
                col_keep_no_prefix = ["seq_id", "aligner_taxid_lca"]
                col_keep_add_prefix = ["genome_id_all"]
                column_prefix = "prim_align_"
            }
            workflow {
                """
                input[0] = GZIP_LCA_EMPTY.out
                input[1] = GZIP_BOWTIE_EMPTY.out
                input[2] = params.col_keep_no_prefix
                input[3] = params.col_keep_add_prefix
                input[4] = params.column_prefix
                """
            }
        }

        then {
            // Should run without failures
            assert workflow.success
            
            // Output file should exist but be empty
            def outputFile = path(workflow.out.output[0][1])
            assert outputFile.exists()
            
            // Check that the file is empty (no lines when decompressed)
            def outputLines = outputFile.linesGzip.size()
            assert outputLines == 0
        }
    }
}
