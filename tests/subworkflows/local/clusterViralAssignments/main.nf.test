nextflow_workflow {

    name "Test workflow CLUSTER_VIRAL_ASSIGNMENTS"
    script "subworkflows/local/clusterViralAssignments/main.nf"
    workflow "CLUSTER_VIRAL_ASSIGNMENTS"
    config "tests/configs/downstream.config"
    tag "subworkflow"
    tag "downstream"
    tag "cluster_viral_assignments"

    test("Should run without failures on interleaved input data"){
        tag "expect_success"
        tag "interleaved"
        setup {
            run("LOAD_SAMPLESHEET") {
                script "subworkflows/local/loadSampleSheet/main.nf"
                process {
                    """
                    input[0] = "${projectDir}/test-data/samplesheet.csv"
                    input[1] = "illumina"
                    input[2] = false
                    """
                }
            }
            run("INTERLEAVE_FASTQ") {
                script "modules/local/interleaveFastq/main.nf"
                process {
                    '''
                    input[0] = LOAD_SAMPLESHEET.out.samplesheet
                    '''
                }
            }
        }
        when {
            params {
                cluster_identity = 0.95
                cluster_min_len = 15
                n_clusters = 10
                single_end = false
            }
            workflow {
                '''
                input[0] = INTERLEAVE_FASTQ.out.output
                input[1] = params.cluster_identity
                input[2] = params.cluster_min_len
                input[3] = params.n_clusters
                input[4] = LOAD_SAMPLESHEET.out.single_end
                '''
            }
        }
        then {
            // Should run without failures
            assert workflow.success
            // Clustering should work as expected
            def fastqs_merged = workflow.out.test_merged.collect{path(it[1]).fastq}
            def fastas_clustered = workflow.out.test_cluster_reps.collect{path(it[1]).fasta}
            def tabs_clustered = workflow.out.test_cluster_summ.collect{path(it[1]).csv(sep: "\t",decompress: true, header: false)}
            def tabs_clustered_out = workflow.out.tsv.collect{path(it[1]).csv(sep: "\t",decompress: true)}
            def tabs_clustered_ids = workflow.out.ids.collect{path(it[1]).csv(header: false)}
            assert fastas_clustered.size() == fastqs_merged.size()
            assert tabs_clustered.size() == fastas_clustered.size()
            assert tabs_clustered_out.size() == tabs_clustered.size()
            assert tabs_clustered_ids.size() == tabs_clustered_out.size()
            for (int i=0; i < fastas_clustered.size(); i++) {
                def n_clusters = tabs_clustered[i].columns["C0"].count("C")
                def n_reps = tabs_clustered[i].columns["C0"].count("S")
                def n_hits = tabs_clustered[i].columns["C0"].count("H")
                def n_seqs = n_reps + n_hits
                assert fastqs_merged[i].sequences.size() == n_seqs
                assert fastas_clustered[i].size() == n_reps
                assert n_reps == n_clusters
                assert tabs_clustered_out[i].rowCount == n_seqs
                assert tabs_clustered_out[i].columns["cluster_id"].unique().size() == n_clusters
                def exp_output_ids = Math.min(n_clusters, params.n_clusters)
                assert tabs_clustered_ids[i].columns["C0"].unique().size() == exp_output_ids
            }
            // Subsetting on representative ID should work as expected
            def fastqs_subset = workflow.out.fastq.collect{path(it[1]).fastq}
            def fastas_subset = workflow.out.fasta.collect{path(it[1]).fasta}
            assert fastqs_subset.size() == fastqs_merged.size()
            assert fastas_subset.size() == fastqs_merged.size()
            for (int i=0; i < fastqs_subset.size(); i++) {
                def ids_in = tabs_clustered_ids[i].columns["C0"].toSorted()
                def names_out = fastqs_subset[i].readNames
                def ids_out = names_out.collect{it.tokenize(" ")[0]}.toSorted()
                def ids_fasta = fastas_subset[i].keySet().collect{it.tokenize(" ")[0]}.toSorted()
                assert ids_in == ids_out.unique()
                assert ids_out == ids_fasta
                assert fastqs_merged[i].readNames.containsAll(names_out)
            }
        }
    }

    test("Should run without failures on single-end input data"){
        tag "expect_success"
        tag "single_end"
        setup {
            run("LOAD_SAMPLESHEET") {
                script "subworkflows/local/loadSampleSheet/main.nf"
                process {
                    """
                    input[0] = "${projectDir}/test-data/single-end-samplesheet.csv"
                    input[1] = "illumina"
                    input[2] = true
                    """
                }
            }
            run("COPY_FILE") {
                script "modules/local/copyFile/main.nf"
                process {
                    """
                    input[0] = LOAD_SAMPLESHEET.out.samplesheet
                    input[1] = "reads.fastq.gz"
                    """
                }
            }
        }
        when {
            params {
                cluster_identity = 0.95
                cluster_min_len = 15
                n_clusters = 10
                single_end = false
            }
            workflow {
                '''
                input[0] = COPY_FILE.out
                input[1] = params.cluster_identity
                input[2] = params.cluster_min_len
                input[3] = params.n_clusters
                input[4] = LOAD_SAMPLESHEET.out.single_end
                '''
            }
        }
        then {
            // Should run without failures
            assert workflow.success
            // Clustering should work as expected
            def fastqs_merged = workflow.out.test_merged.collect{path(it[1]).fastq}
            def fastas_clustered = workflow.out.test_cluster_reps.collect{path(it[1]).fasta}
            def tabs_clustered = workflow.out.test_cluster_summ.collect{path(it[1]).csv(sep: "\t",decompress: true, header: false)}
            def tabs_clustered_out = workflow.out.tsv.collect{path(it[1]).csv(sep: "\t",decompress: true)}
            def tabs_clustered_ids = workflow.out.ids.collect{path(it[1]).csv(header: false)}
            assert fastas_clustered.size() == fastqs_merged.size()
            assert tabs_clustered.size() == fastas_clustered.size()
            assert tabs_clustered_out.size() == tabs_clustered.size()
            assert tabs_clustered_ids.size() == tabs_clustered_out.size()
            for (int i=0; i < fastas_clustered.size(); i++) {
                def n_clusters = tabs_clustered[i].columns["C0"].count("C")
                def n_reps = tabs_clustered[i].columns["C0"].count("S")
                def n_hits = tabs_clustered[i].columns["C0"].count("H")
                def n_seqs = n_reps + n_hits
                assert fastqs_merged[i].sequences.size() == n_seqs
                assert fastas_clustered[i].size() == n_reps
                assert n_reps == n_clusters
                assert tabs_clustered_out[i].rowCount == n_seqs
                assert tabs_clustered_out[i].columns["cluster_id"].unique().size() == n_clusters
                def exp_output_ids = Math.min(n_clusters, params.n_clusters)
                assert tabs_clustered_ids[i].columns["C0"].unique().size() == exp_output_ids
            }
            // Subsetting on representative ID should work as expected
            def fastqs_subset = workflow.out.fastq.collect{path(it[1]).fastq}
            def fastas_subset = workflow.out.fasta.collect{path(it[1]).fasta}
            assert fastqs_subset.size() == fastqs_merged.size()
            assert fastas_subset.size() == fastqs_merged.size()
            for (int i=0; i < fastqs_subset.size(); i++) {
                def ids_in = tabs_clustered_ids[i].columns["C0"].toSorted()
                def names_out = fastqs_subset[i].readNames
                def ids_out = names_out.collect{it.tokenize(" ")[0]}.toSorted()
                def ids_fasta = fastas_subset[i].keySet().collect{it.tokenize(" ")[0]}.toSorted()
                assert ids_in == ids_out.unique()
                assert ids_out == ids_fasta
                assert fastqs_merged[i].readNames.containsAll(names_out)
            }
        }
    }

}
