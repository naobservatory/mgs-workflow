// Helper function: manual table filtering
def filter_tab = { tab_in, threshold -> csv(tab_in.table.where(
    tab_in.table.intColumn("kraken2_assigned_host_virus").isEqualTo(1).or(
        tab_in.table.numberColumn("aligner_length_normalized_score").isGreaterThanOrEqualTo(threshold).and(
            tab_in.table.intColumn("kraken2_assigned_host_virus").isEqualTo(2).or(
                tab_in.table.booleanColumn("kraken2_classified").isFalse()))))) }

nextflow_workflow {

    name "Test subworkflow SUBSET_TRIM"
    script "subworkflows/local/subsetTrim/main.nf"
    workflow "SUBSET_TRIM"
    tag "subworkflow"
    tag "subset_trim"

    test("Should run without failures on paired data") {
        config "tests/configs/run.config"
        tag "paired_end"
        tag "expect_success"
        setup {
            run("LOAD_SAMPLESHEET") {
                script "subworkflows/local/loadSampleSheet/main.nf"
                process {
                    """
                    input[0] = "${projectDir}/test-data/samplesheet.csv"
                    input[1] = "illumina"
                    input[2] = false
                    """
                }
            }
        }
        when {
            params {
                n_reads = 49
            }
            workflow {
                '''
                def subset_trim_params = [
                    n_reads: params.n_reads,
                    adapter_path: params.adapters,
                    platform: "illumina",
                    random_seed: ""
                ]
                input[0] = LOAD_SAMPLESHEET.out.samplesheet
                input[1] = subset_trim_params
                input[2] = LOAD_SAMPLESHEET.out.single_end
                '''
            }
        }
        then {
            // Should run without failures
            assert workflow.success
            // Output should be valid interleaved FASTQ
            def fastq_out_subset = path(workflow.out.subset_reads[0][1]).fastq
            def fastq_out_trimmed = path(workflow.out.trimmed_subset_reads[0][1]).fastq
            def fastq_out_failed = path(workflow.out.test_failed[0][1]).fastq
            def n_reads_subset = fastq_out_subset.getNumberOfRecords()
            def n_reads_trimmed = fastq_out_trimmed.getNumberOfRecords()
            def n_reads_failed = fastq_out_failed.getNumberOfRecords()
            assert n_reads_subset % 2 == 0
            assert n_reads_trimmed % 2 == 0
            assert n_reads_failed % 2 == 0
            // Subset output should approximate expected read count
            // NB: Not exact due to independent sampling
            // TODO: Calculate confidence interval in a more principled way
            def exp_reads_min = params.n_reads * 0.75
            def exp_reads_max = params.n_reads * 1.25
            assert n_reads_subset >= exp_reads_min * 2
            assert n_reads_subset <= exp_reads_max * 2
            // FASTP output should conserve subset reads
            assert n_reads_subset == n_reads_trimmed + n_reads_failed
            def ids_subset = fastq_out_subset.readNames.collect{ it.tokenize(" ")[0] }
            def ids_trimmed = fastq_out_trimmed.readNames.collect{ it.tokenize(" ")[0] }
            def ids_failed = fastq_out_failed.readNames.collect{ it.tokenize(" ")[0] }
            for (id in ids_subset) {
                if (id in ids_trimmed) {
                    assert !(id in ids_failed)
                } else {
                    assert (id in ids_failed)
                }
            }
        }
    }

    test("Should run without failures on single-end data") {
        config "tests/configs/run.config"
        tag "single_end"
        tag "expect_success"
        setup {
            run("LOAD_SAMPLESHEET") {
                script "subworkflows/local/loadSampleSheet/main.nf"
                process {
                    """
                    input[0] = "${projectDir}/test-data/single-end-samplesheet.csv"
                    input[1] = "illumina"
                    input[2] = true
                    """
                }
            }
        }
        when {
            params {
                n_reads = 109
            }
            workflow {
                '''
                def subset_trim_params = [
                    n_reads: params.n_reads,
                    adapter_path: params.adapters,
                    platform: "illumina",
                    random_seed: ""
                ]
                input[0] = LOAD_SAMPLESHEET.out.samplesheet
                input[1] = subset_trim_params
                input[2] = LOAD_SAMPLESHEET.out.single_end
                '''
            }
        }
        then {
            // Should run without failures
            assert workflow.success
            // Subset output should approximate expected read count
            // NB: Not exact due to independent sampling
            // TODO: Calculate confidence interval in a more principled way
            def fastq_out_subset = path(workflow.out.subset_reads[0][1]).fastq
            def n_reads_subset = fastq_out_subset.getNumberOfRecords()
            def exp_reads_min = params.n_reads * 0.75
            def exp_reads_max = params.n_reads * 1.25
            assert n_reads_subset >= exp_reads_min
            assert n_reads_subset <= exp_reads_max
            // FASTP output should conserve subset reads
            def fastq_out_trimmed = path(workflow.out.trimmed_subset_reads[0][1]).fastq
            def fastq_out_failed = path(workflow.out.test_failed[0][1]).fastq
            def n_reads_trimmed = fastq_out_trimmed.getNumberOfRecords()
            def n_reads_failed = fastq_out_failed.getNumberOfRecords()
            assert n_reads_subset == n_reads_trimmed + n_reads_failed
            def ids_subset = fastq_out_subset.readNames.collect{ it.tokenize(" ")[0] }
            def ids_trimmed = fastq_out_trimmed.readNames.collect{ it.tokenize(" ")[0] }
            def ids_failed = fastq_out_failed.readNames.collect{ it.tokenize(" ")[0] }
            for (id in ids_subset) {
                if (id in ids_trimmed) {
                    assert !(id in ids_failed)
                } else {
                    assert (id in ids_failed)
                }
            }
        }
    }
    
    test("Should handle empty input files properly") {
        config "tests/configs/run.config"
        tag "empty_input"
        tag "expect_success"
        setup {
            run("GZIP_FILE") {
                script "modules/local/gzipFile/main.nf"
                process {
                    """
                    input[0] = Channel.of("empty_sample").combine(Channel.of("${projectDir}/test-data/toy-data/empty_file.txt"))
                    """
                }
            }
        }
        when {
            params {
                n_reads = 100
            }
            workflow {
                """
                def subset_trim_params = [
                    n_reads: params.n_reads,
                    adapter_path: params.adapters,
                    platform: "illumina",
                    random_seed: ""
                ]
                input[0] = GZIP_FILE.out
                input[1] = subset_trim_params
                input[2] = Channel.of(true)
                """
            }
        }
        then {
            // Should run without failures
            assert workflow.success
            
            // Check if output files exist
            def file_out_subset = path(workflow.out.subset_reads[0][1])
            def file_out_trimmed = path(workflow.out.trimmed_subset_reads[0][1])
            def file_out_failed = path(workflow.out.test_failed[0][1])
            
            assert file_out_subset.exists()
            assert file_out_trimmed.exists()
            assert file_out_failed.exists()
            
            // Check if fastq files are empty
            def fastq_out_subset = file_out_subset.fastq
            def fastq_out_trimmed = file_out_trimmed.fastq
            def fastq_out_failed = file_out_failed.fastq
            
            assert fastq_out_subset.sequences.size() == 0
            assert fastq_out_trimmed.sequences.size() == 0
            assert fastq_out_failed.sequences.size() == 0
        }
    }
}
