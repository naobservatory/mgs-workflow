nextflow_workflow {

    name "Test workflow SPLIT_VIRAL_TSV_BY_SPECIES"
    script "subworkflows/local/splitViralTsvBySpecies/main.nf"
    workflow "SPLIT_VIRAL_TSV_BY_SPECIES"
    config "tests/configs/downstream.config"
    tag "subworkflow"
    tag "downstream"
    tag "split_viral_tsv_by_species"

    test("Should fail when input TSV contains duplicate read IDs") {
        tag "expect_failed"
        tag "duplicate_input"
        setup {
            run("LOAD_DOWNSTREAM_DATA") {
                script "subworkflows/local/loadDownstreamData/main.nf"
                process {
                    """
                    input[0] = "${projectDir}/test-data/downstream/input_file.csv"
                    """
                }
            }
            run("PREPARE_GROUP_TSVS") {
                script "subworkflows/local/prepareGroupTsvs/main.nf"
                process {
                    '''
                    input[0] = LOAD_DOWNSTREAM_DATA.out.input
                    '''
                }
            }
            run("COPY_FILE_BARE") {
                script "modules/local/copyFile/main.nf"
                process {
                    '''
                    input[0] = "${params.ref_dir}/results/total-virus-db-annotated.tsv.gz"
                    input[1] = "virus-db.tsv.gz"
                    '''
                }
            }
        }
        when {
            params {
            }
            workflow {
                '''
                input[0] = PREPARE_GROUP_TSVS.out.groups
                input[1] = COPY_FILE_BARE.out
                '''
            }
        }
        then {
            // Should run without failures
            assert workflow.failed
            assert workflow.exitStatus == 1
            assert workflow.errorReport.contains("ValueError: Duplicate value found in field seq_id")
        }
    }

    test("Should run without failures on valid input data") {
        tag "expect_success"
        setup {
            run("LOAD_DOWNSTREAM_DATA") {
                script "subworkflows/local/loadDownstreamData/main.nf"
                process {
                    """
                    input[0] = "${projectDir}/test-data/downstream/input_file_single.csv"
                    """
                }
            }
            run("PREPARE_GROUP_TSVS") {
                script "subworkflows/local/prepareGroupTsvs/main.nf"
                process {
                    '''
                    input[0] = LOAD_DOWNSTREAM_DATA.out.input
                    '''
                }
            }
            run("COPY_FILE_BARE") {
                script "modules/local/copyFile/main.nf"
                process {
                    '''
                    input[0] = "${params.ref_dir}/results/total-virus-db-annotated.tsv.gz"
                    input[1] = "virus-db.tsv.gz"
                    '''
                }
            }
        }
        when {
            params {
            }
            workflow {
                '''
                input[0] = PREPARE_GROUP_TSVS.out.groups
                input[1] = COPY_FILE_BARE.out
                '''
            }
        }
        then {
            // Should run without failures
            assert workflow.success
            // Sorting should work as expected
            def tabs_in = workflow.out.test_in.collect{path(it[1]).csv(sep: "\t", decompress: true)}
            def tabs_sort = workflow.out.test_sort.collect{path(it[1]).csv(sep: "\t", decompress: true)}
            assert tabs_sort.size() == tabs_in.size()
            for (int i=0; i < tabs_in.size(); i++) {
                assert tabs_sort[i].rowCount == tabs_in[i].rowCount
                assert tabs_sort[i].columnCount == tabs_in[i].columnCount
                assert tabs_sort[i].columns["aligner_taxid"].collect{ it.toString() } == tabs_in[i].columns["aligner_taxid"].collect{ it.toString() }.toSorted()
            }
            // Joined DB should have expected contents
            def tabs_join = workflow.out.test_join.collect{path(it[1]).csv(sep: "\t",decompress: true)}
            def tab_db = path(workflow.out.test_db[0]).csv(sep: "\t", decompress: true)
            assert tabs_join.size() == tabs_sort.size()
            for (int i=0; i < tabs_sort.size(); i++) {
                assert tabs_join[i].rowCount == tabs_sort[i].rowCount
                assert tabs_join[i].columnNames == tabs_sort[i].columnNames + ["taxid_species"]
                for (c in tabs_join[i].columnNames) {
                    if (c in tabs_sort[i].columnNames) {
                        assert tabs_sort[i].columns[c] == tabs_join[i].columns[c]
                    } else {
                        assert c in tab_db.columnNames
                    }
                }
            }
            // Partition should produce expected number of output DBs
            def n_parts_total = 0
            def n_parts_exp
            def n_parts_obs
            for (int i=0; i < tabs_join.size(); i++) {
                n_parts_exp = tabs_join[i].columns["taxid_species"].unique().size()
                n_parts_obs = workflow.out.test_part[i][1].size()
                assert n_parts_exp == n_parts_obs
                n_parts_total += n_parts_obs
            }
            assert workflow.out.tsv.size() == n_parts_total
            // FASTQ extraction and merging should work as expected
            def tabs_part_2 = workflow.out.tsv.collect{path(it[1]).csv(sep: "\t",decompress: true)}
            def fastqs_interleaved = workflow.out.fastq.collect{path(it[1]).fastq}
            for (int i=0; i < tabs_part_2.size(); i++) {
                assert tabs_part_2[i].rowCount == fastqs_interleaved[i].sequences.size() / 2
            }
        }
    }

}
