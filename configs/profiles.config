// Currently universal settings
docker.enabled = true
wave.enabled = true
aws.client.maxConnections = 1000
aws.client.maxErrorRetry = 10
aws.client.connectionTimeout = 0
aws.client.socketTimeout = 0
nextflow.enable.moduleBinaries = true

params {
    db_download_timeout = 1200 // Timeout in seconds for database downloads (default: 20 minutes)
}

// Workflow run profiles
profiles {
    standard { // Run on AWS Batch
        fusion.enabled = true
        fusion.exportStorageCredentials = true
        process.executor = "awsbatch"
        process.errorStrategy = "retry"
        process.maxRetries = 3
        aws.batch.volumes = ['/scratch:/scratch'] // Shared directory for large reference files
    }
    batch { // Run on AWS Batch
        fusion.enabled = true
        fusion.exportStorageCredentials = true
        process.executor = "awsbatch"
        process.errorStrategy = "retry"
        process.maxRetries = 3
        aws.batch.volumes = ['/scratch:/scratch'] // Shared directory for large reference files
    }
    ec2_local { // Run on EC2 instance with a local working directory
        fusion.enabled = false
        process.errorStrategy = "finish"
        docker.runOptions = '-e AWS_ACCESS_KEY_ID=$AWS_ACCESS_KEY_ID -e AWS_SECRET_ACCESS_KEY=$AWS_SECRET_ACCESS_KEY'
        // exporting AWS credentials to the container; this is required to make bin/download-db.sh work with ec2_local profile
    }
    ec2_s3 { // Run on EC2 instance with an S3 working directory
        fusion.enabled = true
        fusion.exportStorageCredentials = true
        process.errorStrategy = "retry"
    }
}

// Set working directory
workDir = "${params.base_dir}/work"
